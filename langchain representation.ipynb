{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7882ff57-6633-403f-86c6-b65048970be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e08cb1-ab61-4bda-b1f8-c74e8eac8054",
   "metadata": {},
   "source": [
    "<h1>Embedding</h1>\n",
    "\n",
    "<p>Embeddings are vector representations of words, sentences, or even entire documents. They allow machines to process and understand the meaning and context of text data in a numerical form, which is essential for machine learning and deep learning models like Large Language Models (LLMs).</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dffc8bde-de3f-4b70-81eb-d80ec5ee27b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"embeding.png\" width=\"1000\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<img src=\"embeding.png\" width=\"1000\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3e4bf9-ab07-4666-a66a-9b7c5ce83c0d",
   "metadata": {},
   "source": [
    "<h1>Transformers</h1>\n",
    "\n",
    "<p>The Transformer architecture, introduced in the paper <em>\"Attention is All You Need\"</em> by Vaswani et al. (2017), revolutionized Natural Language Processing (NLP). It uses self-attention mechanisms to process input data efficiently and achieve state-of-the-art performance.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ebeee2af-8645-49e7-957c-ee7a0e9caccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"transformer.png\" width=\"1000\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<img src=\"transformer.png\" width=\"1000\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33aa5d6-0be3-4c8f-8bdb-5e745342bd54",
   "metadata": {},
   "source": [
    "<h3>Here, we’ll explore the three types of Transformer-based architectures:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a163bb7-ba00-4805-ba14-2343d0a04a42",
   "metadata": {},
   "source": [
    "<h2>1. Decoder-Only Models</h2>\n",
    "\n",
    "<h3>Overview:</h3>\n",
    "<p>These models consist of only the decoder part of the Transformer architecture. They are used primarily for language generation tasks, where the model generates text token by token based on input prompts.</p>\n",
    "\n",
    "<h3>Key Features:</h3>\n",
    "<ul>\n",
    "    <li><strong>Causal Masking:</strong> To prevent the model from \"cheating\" (i.e., looking ahead), a causal mask ensures that predictions for a token only depend on previous tokens.</li>\n",
    "    <li><strong>Self-Attention:</strong> Computes relationships between tokens in the input.</li>\n",
    "    <li><strong>Output:</strong> Each token is generated sequentially.</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Architecture:</h3>\n",
    "<p>The decoder uses:</p>\n",
    "<ul>\n",
    "    <li>Self-Attention (with causal masking)</li>\n",
    "    <li>Feedforward neural network</li>\n",
    "    <li>Layer normalization</li>\n",
    "    <li>Positional encoding</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Example Models:</h3>\n",
    "<ul>\n",
    "    <li><strong>GPT</strong> (Generative Pre-trained Transformer): OpenAI’s GPT models (GPT-2, GPT-3, GPT-4).</li>\n",
    "    <li><strong>Claude</strong> by Anthropic.</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Applications:</h3>\n",
    "<ul>\n",
    "    <li>Text generation</li>\n",
    "    <li>Chatbots</li>\n",
    "    <li>Code generation</li>\n",
    "</ul>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99abdb7f-12f8-4373-a9eb-be99969b2cd5",
   "metadata": {},
   "source": [
    "<h2>2. Encoder-Only Models</h2>\n",
    "\n",
    "<h3>Overview:</h3>\n",
    "<p>These models consist of only the encoder part of the Transformer architecture. They are used for tasks that involve understanding the input text, rather than generating new text.</p>\n",
    "\n",
    "<h3>Key Features:</h3>\n",
    "<ul>\n",
    "    <li><strong>Bidirectional Attention:</strong> The encoder allows the model to look at both past and future tokens in the input simultaneously.</li>\n",
    "    <li><strong>Self-Attention:</strong> Computes relationships between all input tokens.</li>\n",
    "    <li><strong>Output:</strong> Produces contextual embeddings for each token in the input.</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Architecture:</h3>\n",
    "<p>The encoder uses:</p>\n",
    "<ul>\n",
    "    <li>Multi-head self-attention (bidirectional)</li>\n",
    "    <li>Feedforward neural network</li>\n",
    "    <li>Layer normalization</li>\n",
    "    <li>Positional encoding</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Example Models:</h3>\n",
    "<ul>\n",
    "    <li><strong>BERT</strong> (Bidirectional Encoder Representations from Transformers): Google’s BERT models.</li>\n",
    "    <li><strong>RoBERTa</strong> (a robustly optimized version of BERT).</li>\n",
    "    <li><strong>DistilBERT</strong> (lightweight version of BERT).</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Applications:</h3>\n",
    "<ul>\n",
    "    <li>Text classification (e.g., sentiment analysis)</li>\n",
    "    <li>Named entity recognition (NER)</li>\n",
    "    <li>Question answering (extractive)</li>\n",
    "    <li>Sentence embeddings</li>\n",
    "</ul>\n",
    "i>Sentence embeddings</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00a75b8-7692-4727-a8c9-4ed0a3481081",
   "metadata": {},
   "source": [
    "<h2>3. Encoder-Decoder Models</h2>\n",
    "\n",
    "<h3>Overview:</h3>\n",
    "<p>These models use both the encoder and decoder parts of the Transformer architecture. The encoder processes the input and generates intermediate representations, while the decoder generates output based on these representations.</p>\n",
    "\n",
    "<h3>Key Features:</h3>\n",
    "<ul>\n",
    "    <li><strong>Encoder:</strong> Encodes the input sequence into a contextual representation.</li>\n",
    "    <li><strong>Decoder:</strong> Generates the output sequence based on the encoder’s representation.</li>\n",
    "    <li><strong>Cross-Attention:</strong> The decoder attends to the output of the encoder.</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Architecture:</h3>\n",
    "<ul>\n",
    "    <li><strong>Encoder:</strong> Processes the input sequence with multi-head self-attention and feedforward layers.</li>\n",
    "    <li><strong>Decoder:</strong> Includes two types of attention:\n",
    "        <ul>\n",
    "            <li><strong>Self-Attention:</strong> Uses causal masking.</li>\n",
    "            <li><strong>Cross-Attention:</strong> Attends to the encoder’s output.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<h3>Example Models:</h3>\n",
    "<ul>\n",
    "    <li><strong>T5</strong> (Text-to-Text Transfer Transformer): Google’s T5 treats every task as a text-to-text problem.</li>\n",
    "    <li><strong>BART:</strong> Combines BERT’s bidirectional encoder with GPT’s autoregressive decoder.</li>\n",
    "    <li><strong>mT5 / MarianMT:</strong> Used for multilingual translation tasks.</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Applications:</h3>\n",
    "<ul>\n",
    "    <li>Machine translation (e.g., English to French)</li>\n",
    "    <li>Summarization</li>\n",
    "    <li>Text-to-text generation</li>\n",
    "    <li>Conversational AI</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd63661-0353-4892-837c-06cf4137021a",
   "metadata": {},
   "source": [
    "<h1>Retrieval-Augmented Generation (RAG)</h1>\n",
    "<p>\n",
    "    In Retrieval-Augmented Generation (RAG), the system utilizes a large language model (LLM) to generate responses based on retrieved information. When a user poses a question, the system first retrieves relevant documents or knowledge from a database or external sources. These retrieved data are then provided as input to the LLM, which processes the information and generates a coherent response.\n",
    "</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c806f497-e31f-4bf1-9892-0f88f8468753",
   "metadata": {},
   "source": [
    "HTML('<img src=\"rag.png\" width=\"1000\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a7cbf2-8c2e-4efa-b28e-6e5d7495347c",
   "metadata": {},
   "source": [
    "# LangChain Overview\n",
    "\n",
    "- **Introduction**: LangChain is an open-source framework created in October 2022 by Harrison Chase for integrating large language models (LLMs) into applications.\n",
    "- **Popularity**: Over 41,900 GitHub stars and 800+ contributors by September 2023.\n",
    "- **Funding**: Secured $20M from Sequoia Capital in April 2023, valuing the company at $200M, following $10M funding from Benchmark.\n",
    "- **Features**: Provides tools for building chatbots, Q&A systems, text summarization, and code analysis.\n",
    "- **Programming Language Support**: LangChain supports both **Python** and **JavaScript**, making it versatile for developers across platforms.\n",
    "- **Community**: A strong, supportive community of developers is actively contributing and collaborating.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294c690d-8c8a-4646-95e3-35e0c4faf409",
   "metadata": {},
   "source": [
    "<h1>1. What are Models in LangChain?</h1>\n",
    "<h2>Definition:</h2>\n",
    "<p>Models in LangChain refer to the LLMs or other machine learning models that perform the core language processing tasks.</p>\n",
    "<h2>Functionality:</h2>\n",
    "<p>They generate responses, embeddings, or insights based on input text and are configured to work seamlessly with LangChain workflows.</p>\n",
    "\n",
    "<h1>2. Types of Models in LangChain</h1>\n",
    "<p>LangChain supports various types of models, broadly categorized into:</p>\n",
    "\n",
    "<h2>a. Language Models</h2>\n",
    "<ul>\n",
    "    <li>These are general-purpose models used for text generation, completion, and reasoning tasks.</li>\n",
    "    <li><strong>Examples:</strong> OpenAI GPT, Hugging Face models, Cohere.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>b. Embedding Models</h2>\n",
    "<ul>\n",
    "    <li>Models that generate embeddings (vector representations) for text.</li>\n",
    "    <li>Used in tasks like semantic search, similarity matching, and clustering.</li>\n",
    "    <li><strong>Examples:</strong> OpenAI Embedding API (<code>text-embedding-ada-002</code>), SentenceTransformers.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>c. Chat Models</h2>\n",
    "<ul>\n",
    "    <li>Models specifically designed for conversational tasks, maintaining multi-turn dialogue.</li>\n",
    "    <li><strong>Examples:</strong> OpenAI’s ChatGPT models.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955e9a5e-99cb-4859-9999-aab9348df9a7",
   "metadata": {},
   "source": [
    "<h1>Prompt Component in LangChain</h1>\n",
    "<p>The Prompt component in LangChain is a foundational part of how the framework interacts with Large Language Models (LLMs). Prompts define the input that is sent to the model, shaping its behavior and responses.</p>\n",
    "\n",
    "<h2>What is a Prompt in LangChain?</h2>\n",
    "<p>A prompt is a carefully designed input that guides the model to generate desired outputs. Prompts can be:</p>\n",
    "<ul>\n",
    "  <li><strong>Static:</strong> Predefined and unchanging.</li>\n",
    "  <li><strong>Dynamic:</strong> Constructed based on user input or context.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Applications of Prompts</h2>\n",
    "<ul>\n",
    "  <li>\n",
    "    <h3>Text Summarization:</h3>\n",
    "    <p><strong>Prompt:</strong> \"Summarize the following text in 100 words: {text}\"</p>\n",
    "  </li>\n",
    "  <li>\n",
    "    <h3>Question Answering:</h3>\n",
    "    <p><strong>Prompt:</strong> \"Answer the following question: {question}\"</p>\n",
    "  </li>\n",
    "  <li>\n",
    "    <h3>Creative Writing:</h3>\n",
    "    <p><strong>Prompt:</strong> \"Write a poem about {topic} in the style of Shakespeare.\"</p>\n",
    "  </li>\n",
    "  <li>\n",
    "    <h3>Code Generation:</h3>\n",
    "    <p><strong>Prompt:</strong> \"Generate Python code to calculate {math_function}.\"</p>\n",
    "  </li>\n",
    "  <li>\n",
    "    <h3>Chatbots:</h3>\n",
    "    <p>Use multi-turn chat prompts to create conversational agents.</p>\n",
    "  </li>\n",
    "</ul>\n",
    "\n",
    "<h2>Best Practices for Designing Prompts</h2>\n",
    "<ul>\n",
    "  <li>\n",
    "    <h3>Be Clear and Specific:</h3>\n",
    "    <p>Avoid vague instructions. Specify the task and constraints.</p>\n",
    "    <p><strong>Example:</strong> \"Summarize the text in 3 bullet points\" is better than \"Summarize this.\"</p>\n",
    "  </li>\n",
    "  <li>\n",
    "    <h3>Use Examples (Few-Shot Learning):</h3>\n",
    "    <p>Provide examples of input-output pairs to guide the model.</p>\n",
    "  </li>\n",
    "  <li>\n",
    "    <h3>Iterate and Test:</h3>\n",
    "    <p>Refine prompts based on output quality.</p>\n",
    "  </li>\n",
    "  <li>\n",
    "    <h3>Use Variables in Templates:</h3>\n",
    "    <p>For dynamic prompts, use placeholders for input customization.</p>\n",
    "  </li>\n",
    "  <li>\n",
    "    <h3>Leverage Prompt Selectors:</h3>\n",
    "    <p>Dynamically choose appropriate prompts based on the task.</p>\n",
    "  </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6984bd8b-2e8d-4b90-8ead-a68bfa64eb77",
   "metadata": {},
   "source": [
    "<h1>Loaders in LangChain</h1>\n",
    "<p>Loaders in LangChain are responsible for importing and processing data from various sources, such as documents, web pages, databases, or APIs, into a format suitable for use with Large Language Models (LLMs). These processed data sources are often the starting point for building workflows in LangChain, such as question-answering systems, summarization tasks, or knowledge retrieval pipelines.</p>\n",
    "\n",
    "<h2>1. Purpose of Loaders</h2>\n",
    "<ul>\n",
    "  <li>To read and preprocess data from various file formats or external sources.</li>\n",
    "  <li>To make the data compatible with downstream tasks (e.g., chunking, embedding).</li>\n",
    "  <li>To support integration with workflows like Retrieval Augmented Generation (RAG).</li>\n",
    "</ul>\n",
    "\n",
    "<h2>2. Types of Loaders in LangChain</h2>\n",
    "<p>LangChain provides loaders for a wide range of data sources:</p>\n",
    "\n",
    "<h3>a. File-Based Loaders</h3>\n",
    "<p>Loaders that read and process files in different formats:</p>\n",
    "<ul>\n",
    "  <li><strong>Text Files:</strong> (.txt)</li>\n",
    "  <li><strong>PDF Files:</strong> (.pdf)</li>\n",
    "  <li><strong>Word Documents:</strong> (.docx)</li>\n",
    "  <li><strong>CSV Files:</strong> (.csv)</li>\n",
    "  <li><strong>Markdown Files:</strong> (.md)</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870b2e6b-80d7-4408-99ee-2566c5cd1854",
   "metadata": {},
   "source": [
    "<h1>Indexes in LangChain</h1>\n",
    "<p>In LangChain, indexes are data structures used to organize, store, and retrieve information efficiently. They enable tasks like semantic search, retrieval-augmented generation (RAG), and question-answering by organizing textual data in a way that can be queried using embeddings or keywords.</p>\n",
    "\n",
    "<h2>Purpose of Indexes</h2>\n",
    "<ul>\n",
    "  <li>Organize large amounts of data (documents, text chunks, etc.).</li>\n",
    "  <li>Support efficient querying for tasks like:\n",
    "    <ul>\n",
    "      <li>Retrieving relevant chunks for a prompt.</li>\n",
    "      <li>Finding documents based on semantic similarity.</li>\n",
    "      <li>Filtering results based on metadata.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "</ul>\n",
    "\n",
    "<h2>2. Types of Indexes in LangChain</h2>\n",
    "<p>LangChain supports several types of indexes, depending on the use case:</p>\n",
    "\n",
    "<h3>a. Vector Indexes</h3>\n",
    "<p>Vector indexes use embeddings to represent documents or chunks as vectors in a high-dimensional space. Queries are processed by calculating similarities (e.g., cosine similarity) between the query vector and stored vectors.</p>\n",
    "\n",
    "<h4>Example Use Cases:</h4>\n",
    "<ul>\n",
    "  <li>Semantic search.</li>\n",
    "  <li>Retrieval-augmented generation (RAG).</li>\n",
    "</ul>\n",
    "\n",
    "<h4>Example Libraries:</h4>\n",
    "<ul>\n",
    "  <li><strong>FAISS:</strong> Facebook AI Similarity Search.</li>\n",
    "  <li><strong>Weaviate, Pinecone, Milvus:</strong> Vector database services.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df06c504-a2d0-46cd-a2ca-d33ebbe3d78d",
   "metadata": {},
   "source": [
    "<h1>Tools in LangChain</h1>\n",
    "<p>In LangChain, Tools represent external resources or functionalities that the language model can invoke to extend its capabilities. These tools bridge the gap between the model and external systems, enabling it to perform tasks like database queries, API calls, calculations, or real-time searches.</p>\n",
    "\n",
    "<h2>What Are Tools?</h2>\n",
    "<ul>\n",
    "  <li><strong>Definition:</strong> Tools are external functionalities that the model can call when it requires assistance with tasks beyond its inherent knowledge or reasoning capabilities.</li>\n",
    "  <li><strong>Purpose:</strong> To enhance the model’s functionality by enabling it to:\n",
    "    <ul>\n",
    "      <li>Access real-time information (e.g., web search).</li>\n",
    "      <li>Perform computations.</li>\n",
    "      <li>Interact with databases, APIs, or custom functions.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "</ul>\n",
    "\n",
    "<h2>Why Are Tools Important?</h2>\n",
    "<p>LLMs, while powerful, have certain limitations:</p>\n",
    "<ul>\n",
    "  <li><strong>Static Knowledge:</strong> Cannot access up-to-date or external information.</li>\n",
    "  <li><strong>Inability to Act:</strong> Cannot perform tasks like sending emails or querying databases.</li>\n",
    "  <li><strong>Lack of Precision:</strong> May produce inaccurate answers for specific calculations or technical tasks.</li>\n",
    "</ul>\n",
    "<p>Tools solve these limitations by enabling the model to take external actions.</p>\n",
    "\n",
    "<h2>Examples of Tools in LangChain</h2>\n",
    "<ul>\n",
    "  <li>\n",
    "    <h3>a. Search Tools</h3>\n",
    "    <p>Access real-time information via search engines.</p>\n",
    "    <p><strong>Example:</strong> Google Search, Bing API.</p>\n",
    "  </li>\n",
    "  <li>\n",
    "    <h3>b. Database Tools</h3>\n",
    "    <p>Query structured or unstructured databases.</p>\n",
    "    <p><strong>Example:</strong> SQL databases, MongoDB, or vector databases like FAISS.</p>\n",
    "  </li>\n",
    "  <li>\n",
    "    <h3>c. Calculation Tools</h3>\n",
    "    <p>Perform precise mathematical calculations or evaluate code snippets.</p>\n",
    "    <p><strong>Example:</strong> Python code evaluation tools.</p>\n",
    "  </li>\n",
    "  <li>\n",
    "    <h3>d. APIs</h3>\n",
    "    <p>Interact with external APIs for services like weather, stock prices, or custom applications.</p>\n",
    "  </li>\n",
    "  <li>\n",
    "    <h3>e. Custom Functions</h3>\n",
    "    <p>User-defined Python functions that the model can call for specialized tasks.</p>\n",
    "  </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98492777-2b14-461b-8bc7-71ace01162f7",
   "metadata": {},
   "source": [
    "<h1>Agents in LangChain</h1>\n",
    "<p>Agents in LangChain are dynamic systems that can decide which tools to use and in what sequence based on user queries. They enable complex, multi-step reasoning and task execution by combining the power of Large Language Models (LLMs) with external tools, APIs, or custom logic.</p>\n",
    "\n",
    "<h2>What Are Agents?</h2>\n",
    "<ul>\n",
    "  <li><strong>Definition:</strong> Agents are decision-making components that can interact with tools and decide how to solve a task dynamically.</li>\n",
    "  <li><strong>Purpose:</strong> To handle open-ended tasks where the sequence of actions or tools required isn’t predefined.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Why Use Agents?</h2>\n",
    "<p>Agents are useful when:</p>\n",
    "<ul>\n",
    "  <li>The task requires multiple tools or steps to complete.</li>\n",
    "  <li>The user’s query is dynamic or ambiguous, requiring reasoning to choose the right approach.</li>\n",
    "  <li>External systems (e.g., databases, search engines, calculators) need to be queried to generate accurate responses.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a102afed-731c-44fb-8c8d-c49877909e66",
   "metadata": {},
   "source": [
    "<h1>Memory in LangChain</h1>\n",
    "<p>Memory in LangChain enables an agent or chatbot to maintain context across multiple interactions, allowing it to generate more coherent and contextually relevant responses. This is especially useful in applications like conversational agents, where retaining the context of a dialogue is essential for a seamless user experience.</p>\n",
    "\n",
    "<h2>What is Memory in LangChain?</h2>\n",
    "<ul>\n",
    "  <li><strong>Definition:</strong> Memory stores information about past interactions or specific data points, which the agent can access during subsequent queries.</li>\n",
    "  <li><strong>Purpose:</strong> To help the agent maintain a state or conversational context across multiple turns or interactions.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Why Use Memory?</h2>\n",
    "<ul>\n",
    "  <li><strong>Improved Conversation Flow:</strong> The agent can reference earlier parts of the dialogue.</li>\n",
    "  <li><strong>Personalization:</strong> Store user preferences or recurring details for tailored interactions.</li>\n",
    "  <li><strong>Coherence:</strong> Avoid asking repetitive questions or providing inconsistent responses.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41d7132-24fd-4b38-82ae-f8b57525b90e",
   "metadata": {},
   "source": [
    "<h1>Chains in LangChain</h1>\n",
    "<p>Chains in LangChain are sequences of operations that combine Large Language Models (LLMs), tools, and prompts to execute complex tasks. They allow you to create workflows where the output of one step becomes the input for the next, enabling multi-step reasoning and task execution.</p>\n",
    "\n",
    "<h2>What Are Chains?</h2>\n",
    "<ul>\n",
    "  <li><strong>Definition:</strong> A Chain is a structured workflow that connects multiple components (e.g., prompts, LLMs, tools, memory) to solve a task.</li>\n",
    "  <li><strong>Purpose:</strong> Simplify complex workflows by breaking them into smaller, reusable, and modular steps.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Why Use Chains?</h2>\n",
    "<p>Chains are helpful when:</p>\n",
    "<ul>\n",
    "  <li>A task requires multiple steps (e.g., question answering + summarization).</li>\n",
    "  <li>You need a modular, reusable structure for workflows.</li>\n",
    "  <li>Tasks involve interactions between LLMs, tools, and data (e.g., RAG workflows).</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Components of a Chain</h2>\n",
    "<p>Chains typically consist of:</p>\n",
    "<ul>\n",
    "  <li><strong>Prompts:</strong> Instructions for the LLM at each step.</li>\n",
    "  <li><strong>LLMs:</strong> Generate outputs based on prompts.</li>\n",
    "  <li><strong>Memory:</strong> Store and retrieve context across steps.</li>\n",
    "  <li><strong>Tools:</strong> External functionalities, such as search or calculations.</li>\n",
    "  <li><strong>Input/Output Parsers:</strong> Format and process inputs and outputs.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c65211e-f3d9-46c9-a715-a73a966e2079",
   "metadata": {},
   "source": [
    "<h2>local language model instance and invokes it to generate a response</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "57944511-cc4d-4373-9016-3750dbddc4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ید.\\nموسیقی ایرانی از جمله موسیقی\\u200cهای سنتی و کلاسیک جهان است که در ایران باستان به وجود آمد. این سبک موسیقی، بر اساس آواز، ساز، شعر و ادبیات ملی ایران، در طول تاریخ با تجدیدنظرهای متعددی مواجه بوده است. موسیقی ایرانی شامل مجموعه\\u200cای از عناصر موسیقایی مانند همخوانی گروهی (چهره)، نوای سازها، آواز، شعر و ادبیات می\\u200cشود.\\nموسیقی ایرانی یک نوع موسیقی سنتی، فرهنگی و ملی ایران است که در طول تاریخ با تجدیدنظرهای مختلفی مواجه شده است. این سبک موسیقی از زمان\\u200cهای دیرینهٔ ایران باستان وجود داشته و در شکل\\u200cگیری آن، تأثیراتی از سایر فرهنگ\\u200cها، مانند یونان، رومی\\u200cها و اسلامی دیده می\\u200cشود.\\nموسیقی ایرانی را می\\u200cتوان به سه بخش کلی تقسیم کرد: ۱) موسيقي پيشدادي (پیشدادی)، که از دوران پیشدادیان گرفته تا ساسانیان ادامه داشت. ۲) موسیقی اسلامی که پس از ورود اسلام در ایران شکل گرفت و به تدریج بر موسیقی سنتی تأثیر گذاشت. ۳'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(\n",
    "    base_url=\"http://192.168.0.10:1234/v1\",  # Pointing to your local server\n",
    "    api_key=\"lm-studio\",  # API key for the local instance\n",
    "    model=\"meta-llama-3.1-8b-instruct\",  # Specify your model identifier\n",
    "    max_tokens= 250\n",
    ")\n",
    "\n",
    "llm.invoke(\"در مورد موسیقی ایرانی توضیح بده\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5cc26e-2e51-44e2-aea4-4d886fbf6d42",
   "metadata": {},
   "source": [
    "<h2>sentiment Analyze</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c485b09-25c0-433a-9c75-5a9b5cc1087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "This is a movie review sentiment classifier.  \n",
    "Review: \"This film was absolutely fantastic!\" This review is positive.  \n",
    "Review: \"It was alright, nothing special though.\" This review is neutral.  \n",
    "Review: \"Terrible acting and a boring plot. I regret watching it.\" This review is negative.  \n",
    "Review: \"خیلی خسته‌کننده بود\" . this review is negative.  \n",
    "Review: \"عالی بود، خیلی لذت بردم\" . this review is positive.  \n",
    "input review:{input_text}  \"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(template = template , input_variables= [\"input_text\"])\n",
    "prompt_1 = prompt.format(input_text = \"افتضاح خوب بود\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6eaad152-e8fe-45e1-b05f-2b9646780cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    base_url=\"http://192.168.0.10:1234/v1\",  # Pointing to your local server\n",
    "    api_key=\"lm-studio\",  # API key for the local instance\n",
    "    model=\"meta-llama-3.1-8b-instruct\",  # Specify your model identifier\n",
    "    max_tokens= 5,\n",
    "    temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea7686aa-aeed-4ef0-ba75-619e23a2d0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " output:positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment = llm.invoke(prompt_1)\n",
    "print(sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2be7a2-14a8-47b3-bf24-6705c729f06c",
   "metadata": {},
   "source": [
    "<h2>Chain</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce0d45d1-f82e-4104-9bc1-c70c5900d0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1968362/299728949.py:2: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_text': 'افتضاح خوب بود', 'text': ' output:positive\\n'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "review = \"افتضاح خوب بود\"\n",
    "chain.invoke({\"input_text\":review} , verbos = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94a0cdd6-5fd9-4190-990c-85a811478c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' output:positive\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = prompt|llm|StrOutputParser()\n",
    "chain.invoke({\"input_text\":review})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b73b0ca-7ff4-40a4-9000-48b7c7e51466",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    base_url=\"http://192.168.0.10:1234/v1\",  # Pointing to your local server\n",
    "    api_key=\"lm-studio\",  # API key for the local instance\n",
    "    model=\"meta-llama-3.1-8b-instruct\"  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab7e0bd4-29b7-46e8-bb58-0e5f75a41b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "llm_math = LLMMathChain.from_llm(llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c29da679-f903-4b7d-a62e-e4545d7434b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_math.prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14e10aa-8834-4a88-b51c-9679bca43ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "308054bb-f307-44cd-804a-215f64704e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "What is 551368 divided by 82\u001b[32;1m\u001b[1;3m```text\n",
      "551368 / 82\n",
      "```\n",
      "...numexpr.evaluate(\"551368 / 82\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m6724.0\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is 551368 divided by 82', 'answer': 'Answer: 6724.0'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "example_query = \"What is 551368 divided by 82\"\n",
    "llm_math.invoke(example_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "671ec6e1-555e-4aad-aad3-b74c8c412513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Results for query 'weather change is a global or local issue?':\n",
      "page_content='Climate change is a pressing global issue.'\n",
      "page_content='The economy has been impacted by recent policy changes.'\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Define the model path (use a public model if you don't have one locally)\n",
    "local_model_path = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=local_model_path)\n",
    "\n",
    "# Example texts and their embeddings\n",
    "texts = [\n",
    "    \"Artificial intelligence is transforming the world.\",\n",
    "    \"Climate change is a pressing global issue.\",\n",
    "    \"The economy has been impacted by recent policy changes.\",\n",
    "    \"Advancements in technology are accelerating innovation.\",\n",
    "    \"Education is the foundation for a brighter future.\"\n",
    "]\n",
    "\n",
    "doc_embeddings = [embeddings.embed_query(text) for text in texts]\n",
    "\n",
    "# Combine texts and embeddings into a single list of tuples\n",
    "text_embedding_pairs = list(zip(texts, doc_embeddings))\n",
    "\n",
    "# Create the FAISS index\n",
    "faiss_index = FAISS.from_embeddings(text_embedding_pairs, embedding=embeddings)\n",
    "\n",
    "# Perform similarity search\n",
    "query = \"weather change is a global or local issue?\"\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "# Use similarity_search_by_vector to directly search using the embedding\n",
    "results = faiss_index.similarity_search_by_vector(query_embedding, k=2)\n",
    "\n",
    "# Print results\n",
    "print(f\"Search Results for query '{query}':\")\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fa466b38-222d-46e4-b900-1eff8316ea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "User Query:\n",
    "{user_query}\n",
    "\n",
    "Relevant Information Retrieved:\n",
    "{rag}\n",
    "\n",
    "Generated Answer:\n",
    "Generate response based only on the Relevant Information Retrieved. Avoid adding any external information or assumptions.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template = template , input_variables= [\"user_query\" , \"rag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "829234c2-150a-438c-bb69-4ed39971148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt|llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7a49d98d-19e3-46bb-815d-93c36efbcba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's a global issue. Climate change is considered a global issue as it affects the entire planet, not just specific regions.  This means that everyone around the world contributes to it and will be impacted by its effects.\\n\\nAdditional Info:\\nIf you'd like to know more about climate change or discuss ways to mitigate its impact, I can provide additional information and insights. Just let me know!\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(({\"user_query\":query , \"rag\":results[0].page_content}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
